# 速率限制
## 概述
### 什么是速率限制？
速率限制是 API 对用户或客户端在指定时间段内访问服务器的次数施加的限制。
### 为什么我们有速率限制？
速率限制是 API 的常见做法，其实施有几个不同的原因：

- 它们有助于防止 API 的滥用或误用

	例如，恶意行为者可能会向 API 发出大量请求，试图使其过载或导致服务中断。通过设置速率限制，OpenAI 可以防止此类活动。
- 速率限制有助于确保每个人都能公平地访问 API。

	如果一个人或组织发出过多的请求，则可能会导致其他人或组织的 API 陷入困境。通过限制单个用户可以发出的请求数量，OpenAI 可确保大多数人有机会使用 API，而不会遇到速度下降的情况。
- 速率限制可以帮助 OpenAI 管理其基础设施上的总负载。

	如果对 API 的请求急剧增加，可能会对服务器造成负担并导致性能问题。通过设置速率限制，OpenAI 可以帮助所有用户保持流畅一致的体验。

请完整阅读本文档，以更好地了解 OpenAI 的速率限制系统的工作原理。我们提供了处理常见问题的代码示例和可能的解决方案。建议在填写[速率限制增加请求表](https://docs.google.com/forms/d/e/1FAIpQLSc6gSL3zfHFlL6gNIyUcjkEv29jModHGxg5_XGyr-PrE2LaHw/viewform)之前遵循本指南，并在最后一部分中提供有关如何填写该表的详细信息。

### 我们的 API 的速率限制是多少？
您可以在账户管理页面的[速率限制](https://platform.openai.com/account/rate-limits)部分查看您组织的速率限制。

我们根据使用的特定端点以及您拥有的帐户类型在[组织级别（而不是用户级别）实施速率限制](https://platform.openai.com/docs/guides/production-best-practices)。速率限制以三种方式衡量：

- RPM（每分钟请求数)
- RPD（每天请求数）
- 和TPM（每分钟token数）

下表重点介绍了我们 API 的默认速率限制。


- 免费用户

	||每日请求数 (RPD)|每分钟请求数 (RPM)|每分钟token数 (TPM)
	---|---|---|---|
	Text & Embedding入|200 RPD |3 RPM |150,000 TPM
	Chat|200 RPD |3 RPM |40,000 TPM
	Image|5 张图像/分钟|---|---
	Audio|200 RPD|3 RPM |---
	微调|---|---|---
	
	
	对于前 48 小时内的免费试用用户，我们增加了每日费率限制。正常的 RPM 和 TPM 限制适用，但每个端点每天的请求数也有单独的限制。

	免费试用用户目前也无法访问微调模型。
- 付费用户（前 48 小时）

	||每日请求数 (RPD)|每分钟请求数 (RPM)|每分钟token数 (TPM)
	---|---|---|---|
	Text & Embedding入|2000 RPD |60 RPM |250,000 TPM
	Chat|2000 RPD |20 RPM |60,000 TPM
	GPT-3.5-Turbo-16K|2000 RPD |20 RPM |120,000 TPM
	Image|2000 RPD|50 张图像/分钟|---|
	Audio|2000 RPD|50 RPM |---
	微调|12 jobs 每天|每个模型一次1个job|---
	
	对于前 48 小时内即用即付的用户，我们增加了每日费率限制。正常的 RPM 和 TPM 限制适用，但每个端点每天的请求数也有单独的限制。

	微调模型使用与基础模型相同的共享速率限制容量。
- 付费用户（48小时后）

	||每日请求数 (RPD)|每分钟请求数 (RPM)|每分钟token数 (TPM)
	---|---|---|---|
	Text & Embedding入|--- |3500 RPM |350,000 TPM
	Chat|---  |3500 RPM|90,000 TPM
	GPT-3.5-Turbo-16K|--- |2000 RPM |180,000 TPM
	Image|---|50 张图像/分钟|---|
	Audio|---|50 RPM |---
	微调|24 jobs 每天|每个模型一次1个job|---

	微调模型使用与基础模型[相同的共享速率限制容量](https://platform.openai.com/docs/guides/fine-tuning/how-do-rate-limits-work-on-fine-tuned-models)。
	
值得注意的是，任一选项都可能会达到速率限制，具体取决于首先发生的情况。例如，您可能会向编辑端点发送仅包含 100 个token的 20 个请求，这将满足您的限制，即使您没有在这 20 个请求中发送 150k token。

### GPT-4 速率限制
在 [GPT-4 推出期间](http://openai.com/blog/gpt-4-api-general-availability)，该模型将具有更激进的速率限制以满足需求。您可以在帐户页面的速率限制部分[查看](https://platform.openai.com/account/rate-limits)当前的速率限制。由于容量限制，我们无法满足提高速率限制的请求。我们首先优先考虑对 GPT-4 的一般访问，随后将在容量允许的情况下自动提高速率限制。
### 速率限制如何运作？
如果您的速率限制为每分钟 60 个请求和每分钟 150k 个 token，则您将因达到请求数/分钟上限或用完 token 而受到限制（以先发生者为准）。

例如，如果您的最大请求数/分钟为 60，则您应该能够每秒发送 1 个请求。如果您每 800 毫秒发送 1 个请求，一旦达到速率限制，您只需让程序休眠 200 毫秒即可再发送一个请求，否则后续请求将失败。默认为 3,000 个请求/分钟，客户可以有效地每 20 毫秒或每 0.02 秒发送 1 个请求。
### 标头中的速率限制
除了在[帐户页面](https://platform.openai.com/account/rate-limits)上查看速率限制之外，您还可以查看有关速率限制的重要信息，例如 HTTP 响应标头中的剩余请求、token和其他元数据。

您可能会看到以下标头字段：

FIELD|SAMPLE VALUE|DESCRIPTION
---|---|---
x-ratelimit-limit-requests|60|在耗尽速率限制之前允许的最大请求数。
x-ratelimit-limit-tokens|150000|在耗尽速率限制之前允许的最大token数。
x-ratelimit-remaining-requests|59|在耗尽速率限制之前允许的剩余请求数。
x-ratelimit-remaining-tokens|149984|在耗尽速率限制之前允许的剩余token数量。
x-ratelimit-reset-requests|1秒|速率限制（基于请求）重置为其初始状态的时间。
x-ratelimit-reset-tokens|6分0秒|速率限制（基于token）重置为其初始状态的时间。

### 如果我遇到速率限制错误，会发生什么情况？
速率限制错误如下所示：

	Rate limit reached for default-text-davinci-002 in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min.
如果您达到速率限制，则意味着您在短时间内发出了太多请求，并且 API 将拒绝满足进一步的请求，直到经过指定的时间。
### 速率限制与 max_tokens
我们提供的[每个模型](https://platform.openai.com/docs/models/overview)都有有限数量的 token，可以在发出请求时作为输入传入。您无法增加模型接受的最大 token 数。例如，如果您使用text-ada-001，则可以发送到此模型的最大 token 数为每个请求 2,048 个 token。
## 减少错误
### 我可以采取哪些措施来缓解这种情况？
OpenAI Cookbook 有一个 [Python notebook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb)，解释了如何避免速率限制错误，以及一个用于在批处理 API 请求时保持速率限制的示例 Python 脚本。

在提供编程访问、批量处理功能和自动社交媒体发布时，您还应该谨慎行事 - 考虑只为值得信赖的客户启用这些功能。

为了防止自动和大量滥用，请在指定时间范围内（每日、每周或每月）为单个用户设置使用限制。考虑对超出限制的用户实施硬上限或手动审核流程。
### 使用指数退避(backoff)重试
避免速率限制错误的一种简单方法是使用随机指数退避自动重试请求。使用指数退避重试意味着在遇到速率限制错误时执行短暂睡眠，然后重试不成功的请求。如果请求仍然不成功，则增加睡眠长度并重复该过程。这将持续到请求成功或达到最大重试次数为止。这种方法有很多好处：

- 自动重试意味着您可以从速率限制错误中恢复，而不会崩溃或丢失数据
- 指数退避意味着您可以快速尝试第一次重试，同时如果前几次重试失败，仍然可以从更长的延迟中受益
- 在延迟中添加随机抖动有助于同时重试所有命中。

请注意，不成功的请求会影响您的每分钟限制，因此连续重新发送请求将不起作用。

下面是一些使用指数退避的 Python 解决方案示例。

- 示例#1：使用 Tenacity 库

	Tenacity 是一个 Apache 2.0 许可的通用重试库，用 Python 编写，用于简化向任何事物添加重试行为的任务。要为您的请求添加指数退避，您可以使用 `tenacity.retry` 装饰器。下面的示例使用该 `tenacity.wait_random_exponential` 函数向请求添加随机指数退避
	
		import openai
		from tenacity import (
		    retry,
		    stop_after_attempt,
		    wait_random_exponential,
		)  # for exponential backoff
		 
		@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
		def completion_with_backoff(**kwargs):
		    return openai.Completion.create(**kwargs)
		 
		completion_with_backoff(model="text-davinci-003", prompt="Once upon a time,")
		
	请注意，Tenacity 库是第三方工具，OpenAI 不保证其可靠性或安全性
- 示例#2：使用 backoff library

	另一个为退避和重试提供函数装饰器的 python 库是 [backoff](https://pypi.org/project/backoff/)
	
		import backoff 
		import openai 
		@backoff.on_exception(backoff.expo, openai.error.RateLimitError)
		def completions_with_backoff(**kwargs):
		    return openai.Completion.create(**kwargs)
		 
		completions_with_backoff(model="text-davinci-003", prompt="Once upon a time,")
	与 Tenacity 一样，退避库是第三方工具，OpenAI 不保证其可靠性或安全性
- 示例 3：手动退避实现

	如果您不想使用第三方库，您可以按照以下示例实现自己的退避逻辑：

		# imports
		import random
		import time
		 
		import openai
		 
		# define a retry decorator
		def retry_with_exponential_backoff(
		    func,
		    initial_delay: float = 1,
		    exponential_base: float = 2,
		    jitter: bool = True,
		    max_retries: int = 10,
		    errors: tuple = (openai.error.RateLimitError,),
		):
		    """Retry a function with exponential backoff."""
		 
		    def wrapper(*args, **kwargs):
		        # Initialize variables
		        num_retries = 0
		        delay = initial_delay
		 
		        # Loop until a successful response or max_retries is hit or an exception is raised
		        while True:
		            try:
		                return func(*args, **kwargs)
		 
		            # Retry on specific errors
		            except errors as e:
		                # Increment retries
		                num_retries += 1
		 
		                # Check if max retries has been reached
		                if num_retries > max_retries:
		                    raise Exception(
		                        f"Maximum number of retries ({max_retries}) exceeded."
		                    )
		 
		                # Increment the delay
		                delay *= exponential_base * (1 + jitter * random.random())
		 
		                # Sleep for the delay
		                time.sleep(delay)
		 
		            # Raise exceptions for any errors not specified
		            except Exception as e:
		                raise e
		 
		    return wrapper
		    
		@retry_with_exponential_backoff
		def completions_with_backoff(**kwargs):
		    return openai.Completion.create(**kwargs)
 
 	同样，OpenAI 不保证该解决方案的安全性或效率，但它可以成为您自己的解决方案的良好起点。   

### 减少 max_tokens 以匹配您完成的规模
`max_tokens` 您的速率限制是根据您的请求的字符数计算的 `token` 的最大值和估计数量。尝试将该 `max_tokens` 值设置为尽可能接近您的预期响应大小。
### 批量请求
OpenAI API 对每分钟请求数和每分钟 `token` 数有单独的限制。

如果您达到了每分钟的请求限制，但每分钟的 `token` 有可用容量，则可以通过将多个任务批处理到每个请求中来提高吞吐量。这将使您每分钟处理更多 `token`，特别是对于我们较小的模型。

发送一批提示的工作方式与普通 API 调用完全相同，只不过您将字符串列表而不是单个字符串传递给提示参数。
### 没有批处理的示例
	import openai
	 
	num_stories = 10
	prompt = "Once upon a time,"
	 
	# serial example, with one story completion per request
	for _ in range(num_stories):
	    response = openai.Completion.create(
	        model="curie",
	        prompt=prompt,
	        max_tokens=20,
	    )
	    # print story
	    print(prompt + response.choices[0].text)
    
### 批处理示例
	import openai  # for making OpenAI API requests
	 
	 
	num_stories = 10
	prompts = ["Once upon a time,"] * num_stories
	 
	# batched example, with 10 story completions per request
	response = openai.Completion.create(
	    model="curie",
	    prompt=prompts,
	    max_tokens=20,
	)
	 
	# match completions to prompts by index
	stories = [""] * len(prompts)
	for choice in response.choices:
	    stories[choice.index] = prompts[choice.index] + choice.text
	 
	# print stories
	for story in stories:
	    print(story)
    
警告：响应对象可能不会按照提示的顺序返回完成，因此请始终记住使用索引字段将响应与提示进行匹配。

## 请求增加
### 我什么时候应该考虑申请提高费率限额？
我们的默认速率限制帮助我们最大限度地提高稳定性并防止滥用我们的 API。我们提高限制是为了支持高流量应用程序，因此申请提高速率限制的最佳时机是当您认为自己拥有必要的流量数据来支持提高速率限制的有力理由时。如果没有支持数据，大幅提高速率限制的请求不太可能获得批准。如果您正准备产品上线，请通过10天内分阶段发布的方式获取相关数据。

请记住，费率限制的提高有时可能需要 7-10 天，因此，如果有数据支持您将达到当前增长数字的费率限制，那么尝试提前计划并尽早提交是有意义的。
### 我的速率限制提高请求会被拒绝吗？
速率限制增加请求通常会被拒绝，因为它缺乏证明增加合理性所需的数据。我们在下面提供了数字示例，展示了如何最好地支持速率限制增加请求，并尽力批准所有符合我们安全政策的请求并显示支持数据。我们致力于帮助开发人员利用我们的 API 进行扩展并取得成功。
### 我已经为我的文本/代码 API 实现了指数退避，但仍然遇到此错误。如何提高我的速率限制？
我们理解有限的利率限制可能导致的挫败感，并且我们很乐意提高每个人的违约率。但是，由于[共享容量限制](https://docs.google.com/forms/d/e/1FAIpQLSc6gSL3zfHFlL6gNIyUcjkEv29jModHGxg5_XGyr-PrE2LaHw/viewform)，我们只能批准通过我们的费率限制提高申请表证明有需要的付费客户提高费率限制。为了帮助我们正确评估您的需求，我们要求您在表格的“分享需求证据”部分中提供有关您当前使用情况的统计数据或根据历史用户活动进行的预测。如果此信息不可用，我们建议采用分阶段发布方法。首先按照当前的速率限制向一部分用户发布服务，收集 10 个工作日的使用数据，然后根据该数据提交正式的速率限制提高请求，以供我们审核和批准。

我们将审核您的请求，如果获得批准，我们将在 7-10 个工作日内通知您批准情况。

以下是一些如何填写此表格的示例：

- DALL-E API 示例

	模型|评估token/每分钟|评估请求/每分钟|用户数|需求证据|1小时最大吞吐成本
	---|---|---|---|---|---|---|
	DALL-E API	|N/A|50|1000|我们的应用程序目前正在生产中，根据过去的流量，我们每分钟发出大约 10 个请求	| `$60`
	DALL-E API	|N/A|150|10,000|我们的应用程序在 App Store 中越来越受欢迎，并且我们开始达到速率限制。我们可以将默认限制 50 img/min 提高三倍吗？如果我们需要更多，我们将提交新表格。谢谢！| `$180`
- 语言模型示例

	模型|评估token/每分钟|评估请求/每分钟|用户数|需求证据|1小时最大吞吐成本
		---|---|---|---|---|---|---|
		GPT-3.5-turbo|325,000|4,000|50|我们正在向最初的一组 alpha 测试人员发布该版本，并且需要更高的限制来适应他们的初始使用情况。我们这里有一个指向 Google Drive 的链接，其中显示了分析和 API 使用情况。|`$390`
		GPT-4|750,000|10,000|10,000|我们的申请引起了很多人的兴趣；我们的候补名单上有 50,000 人。我们希望每天向 1,000 人的群组推出，直到达到 50,000 名用户。请参阅此链接，了解过去 30 天内我们当前的代币/分钟流量。这适用于 500 个用户，根据他们的使用情况，我们认为 750,000 个代币/分钟和 10,000 个请求/分钟将是一个很好的起点。|`$900`
- 代码模型示例

	模型|评估token/每分钟|评估请求/每分钟|用户数|需求证据|1小时最大吞吐成本
	---|---|---|---|---|---|---|
	代码-达芬奇-002|150,000	|1,000|15|我们是一群正在写论文的研究人员。我们估计，为了在本月底之前完成我们的研究，我们需要对代码 davinci-002 进行更高的速率限制。这些估计基于以下计算[...]|xxx

请注意，这些示例只是一般用例场景，实际使用率会根据具体实现和使用情况而有所不同。